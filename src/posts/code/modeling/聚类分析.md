---
title: 聚类分析
date: 2024-03-03
icon: circle-info
---

- 聚类是一个将数据集划分为若干组（class）或类(cluster)的过程,并使得同一个组内的数据对象具有较高的相似度;而不同组中的数据对象是不相似的。
- 相似或不相似是基于数据描述属性的取值来确定的，通常利用各数据对象间的距离来进行表示。
- 聚类分析尤其适合用来探讨样本间的相互关联关系从而对一个样本结构做一个初步的评价。

## 什么是好的聚类
一个好的聚类方法将产生以下的高聚类:

- 最大化类内的相似性;

- 最小化类间的相似性。 

聚类结果的质量依靠所使用度量的相似性和它的执行。

聚类方法的质量也可以用它发现一些或所有隐含模式的能力来度量。

**聚类分析有两种**：一种是对椎品的分类，称为Q型，另一种是对变量（指标）的分类，称为R型。
R型聚类分析的主要作用:

1. 不但可以了解个别变量之间的亲疏程度，而且可以了解各个变量组合之间的亲疏程度。
2. 根据变量的分类结果以及它们之间的关系，可以选择主要变量进行Q型聚类分析或回归分析（R2为选择标准） 

**Q型聚类分析的主要作用**:

1. 可以综合利用多个变量的信息对样本进行分析。
2. 分类结果直观，聚类谱系图清楚地表现数值分类结果。
3. 聚类分析所得到的结果比传统分类方法更细致、全面、合理。在课堂上主要讨论Q型聚类分析,Q型聚类常用的统计量是距离.

## 样品间的相似度量-距离

### 常用距离的定义

设有$n$个样品的$p$元观测数据：
$$
x_i=(x_{i1},x_{i2},\cdots,x_{ip})^T,i=1,2,\cdots,n
$$
这时，每个样品可看成$p$元空间的一个点，每两个点之间的距离记为$d(x_i,x_j)$，满足条件：
$$
d(x_i,x_j)\geq0,且d(x_i,x_j)=0当且仅当x_i=x_j\\
d(x_i,x_j)=d(x_j,x_i)\\
d(x_i,x_j)\leq d(x_i,x_k)+d(x_k,x_j)
$$

1. 欧式距离 $d(x_i,x_j)=[\sum\limits_{k=1}^p(x_{ik}-x_{jk})^2]^{\frac{1}{2}} \quad pdist(x)$
2. 绝对距离 $d(x_i,x_j)=\sum\limits_{k=1}^p\vert x_{ik}-x_{jk}\vert \quad pdist(x,'cityblock')$
3. 明氏距离 $d(x_i,x_j)=[\sum\limits_{k=1}^p\vert x_{ik}-x_{jk}\vert^m]^{\frac{1}{m}} \quad pdist(x,'minkowski',r)$
4. 切氏距离 $d(x_i,x_j)=max\vert x_{ik}-x_{jk}\vert \quad max(abs(xi-xj))$
5. 方差加权距离 $d(x_i,x_j)=[\sum\limits_{k=1}^p(x_{ik}-x_{jk})^2/s_k^2]^{\frac{1}{2}} \quad 将原数据标准化以后的欧式距离$
6. 马氏距离 $d(x_i,x_j)=\sqrt{(x_i-x_j)^T\sum^{-1}(x_i-x_j)} \quad pdist(x,'mahal')$
7. 兰氏距离 $d(x_i,x_j)=\frac{1}{p}\sum\limits_{k=1}^p\frac{\vert x_{ik}-x_{jk}\vert}{x_{ik}+x_{jk}}$
8. 杰氏距离 $d(x_i,x_j)=[\sum\limits_{k=1}^{p}(\sqrt{x_{ik}}-\sqrt{ x_{jk}})^2]^\frac{1}{2}$

```matlab
di=pdist(a)%此时计算出各行之间的欧氏距离
D=squareform(d1)%转换为实对称矩阵
S=tril(D)%得到三角阵
```

### 变量间的相似度量-相似系数

当对$p$个指标变量进行聚类时，用相似系数来衡量变量之间的相似程度（关联度），若用$C_{\alpha,\beta}$表示变量之间的相似系数，则应满足：
$$
\vert C_{\alpha \beta}\vert \leq1且C_{\alpha \alpha}=1\\
C_{\alpha \beta}=\pm1，当且仅当\alpha=k\beta,k\neq0\\
C_{\alpha \beta}=C_{\beta \alpha}
$$
相似系数中最常用的是相关系数与夹角余弦

1. 夹角余弦

$$
C_{ij}(1)=\cos{\alpha_{ij}}=\frac{\sum\limits_{t=1}^nx_{ti}x_{tj}}{\sqrt{\sum\limits_{t=1}^nx_{ti}^2}\sqrt{\sum\limits_{t=1}^nx_{tj}^2}}
$$

2. 相关系数

$$
两变量的相关系数定义为：\\
C_{ij}(2)=\cos{\alpha_{ij}}=\frac{\sum\limits_{t=1}^n(x_{ti}-\bar{x_i})(x_{tj}-\bar{x_j})}{\sqrt{\sum\limits_{t=1}^nx_{ti}^2-\bar{x_i}}\sqrt{\sum\limits_{t=1}^nx_{tj}^2-\bar{x_j}}}
$$

```matlab
a=[7.9,39.77,8.49
	7.68,50.37,11.35
	9.42,27.93,8.2]
R=corrcoef(a)%指标之间的相关系数
a1=normc(a)%将a的各列化为单位向量
J=a1'*a1%计算a中各列之间的夹角余弦
```

### 类间距离

设$d_{ij}$表示两个样品$x_i,x_j$之间的距离，$G_p,G_q$分别表示两个类别,各自含有$n_p,n_q$个样品

1. 最短距离$\quad D_{pq}=\underset{i\in G_p,j\in G_q}{min} \ d_{ij}$

即用两类中样品之间的距离最短者作为两类间距离

2. 最长距离$\quad D_{pq}=\underset{i\in G_p,j\in G_q}{max} \ d_{ij}$

即用两类中样品之间的距离最长者作为两类间距离

3. 类平均距离$\quad D_{pq}=\frac{1}{n_pn_q}\underset{i\in G_p}{\sum}\underset{i\in G_q}{\sum}d_{ij}$

即用两类中所有两两样品之间距离的平均作为两类间距离

4. 重心距离$\quad D_{pq}=d(\bar{x}_p,\bar{x}_q)=\sqrt{(\bar{x}_p-\bar{x}_q)^T(\bar{x}_p-\bar{x}_q)}$

其中分别是的重心，这是用两类的重心之间的欧氏距离作为两类间的距离

5. 离差平方和距离$\quad D_{pq}^2=\frac{n_pn_q}{n_p+n_q}(\bar{x}_p-\bar{x}_q)^T(\bar{x}_p-\bar{x}_q)$

显然，离差平方和距离与重心距离的平方成正比

### 谱系聚类法的步骤

1. 选择样本间距离的定义及类间距离的定义;
2. 计算$n$个样本两两之间的距离，得到距离矩阵$D=(d_{ij})$
3. 构造个类，每类只含有一个样本;
4. 合并符合类间距离定义要求的两类为一个新类;
5. 计算新类与当前各类的距离。若类的个数为1，则转到步骤6，否则回到步骤4;
6. 画出聚类图;
7. 决定类的个数和类。

- $n$个样品开始作为$n$​个类，计算两两之间的距离或相似系数，得到实对称矩阵
  $$
  D_0=
  \left[
  \begin{matrix}
   d_{11}      & d_{12}      & \cdots & d_{1n} \\
   d_{21}      & d_{22}      & \cdots & d_{2n} \\
   \vdots      & \vdots      &        & \vdots \\
   d_{n1}      & d_{n2}      & \cdots & d_{nn} \\
  \end{matrix}
  \right]
  $$
  
- 从$D$的非主对角线上找最小（距离）或最大元素（相似系数），设该元素是$D_{pq}$，则将$G_p,G_q$合并成一个新类$G_r=（G_p,G_q）$，在$D$中去掉$G_p,G_q$所在的两行、两列，并加上新类与其余各类之间的距离(或相似系数)，得到$n-1$阶矩阵$D_1$。

- 从$D_1$出发重复上述步骤的做法得到$D_2$,由$D_2$出发重复上述步骤，直到所有样品聚为一个大类为止。
- 在合并过程中要记下合并样品的编号及两类合并时的水平，并绘制聚类谱系图。

**合并后的距离取最小值**

### K-平均聚类算法

#### 算法的特点

- 只适用于聚类均值有意义的场合，在某些应用中，如：数据集中包含符号属性时，直接应用k-means算法就有问题;
- 用户必须事先指定k的个数;
- 对噪声和孤立点数据敏感，少量的该类数据能够对聚类均值起到很大的影响。

#### 方法

选取k个点，让数据集与三个点取欧氏距离比较哪个最小属于哪个类

